{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note to import from .py files, must follow structure\n",
    "# from <.py filename excluding '.py'> import <class name>\n",
    "# Optionslam creds: aspringfastlaner Options2018\n",
    "\n",
    "# Importing necessary models\n",
    "import smtplib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas.stats.moments as st\n",
    "from pandas import ExcelWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as dates\n",
    "# import matplotlib.ticker as ticker\n",
    "from lxml import html\n",
    "import requests\n",
    "import webbrowser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "import csv\n",
    "import sched, time\n",
    "import pandas_datareader as datareader\n",
    "from pandas_datareader.data import Options\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "ts = TimeSeries(key='5HZEUI5AFJB06BUK',output_format='pandas')\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request as urlreq\n",
    "from collections import OrderedDict\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from pandas_datareader.data import Options\n",
    "import py_vollib\n",
    "from py_vollib.black_scholes_merton.implied_volatility import *\n",
    "from py_vollib.black_scholes_merton.greeks.analytical import *\n",
    "\n",
    "'''\n",
    "Calculate the Black-Scholes implied volatility.\n",
    "\n",
    "Parameters:\t\n",
    "price (float) – the Black-Scholes option price\n",
    "S (float) – underlying asset price\n",
    "K (float) – strike price\n",
    "t (float) – time to expiration in years\n",
    "r (float) – risk-free interest rate\n",
    "flag (str) – ‘c’ or ‘p’ for call or put.\n",
    ">>> S = 100\n",
    ">>> K = 100\n",
    ">>> sigma = .2\n",
    ">>> r = .01\n",
    ">>> flag = 'c'\n",
    ">>> t = .5\n",
    ">>> price = black_scholes(flag, S, K, t, r, sigma)\n",
    ">>> iv = implied_volatility(price, S, K, t, r, flag)\n",
    "'''\n",
    "%matplotlib inline\n",
    "\n",
    "def write_excel(filename, sheetnames, df_list):\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "    writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n",
    "    for i, df in enumerate(df_list):\n",
    "        \n",
    "        df.to_excel(writer, sheet_name = sheetnames[i])\n",
    "\n",
    "    # Close the Pandas Excel writer and output the Excel file.\n",
    "    writer.save()\n",
    "    return\n",
    "\n",
    "# Alpha Vantage API Key\n",
    "# 5HZEUI5AFJB06BUK\n",
    "\n",
    "# ts = TimeSeries(key='5HZEUI5AFJB06BUK', output_format='pandas')\n",
    "# data, meta_data = ts.get_intraday(symbol='MSFT',interval='1min', outputsize='full')\n",
    "# data['close'].plot()\n",
    "# plt.title('Intraday Times Series for the MSFT stock (1 min)')\n",
    "# For intraday\n",
    "# https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=MSFT&interval=1min&apikey=d5HZEUI5AFJB06BUK&datatype=csv\n",
    "\n",
    "# For daily\n",
    "# https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=MSFT&apikey=5HZEUI5AFJB06BUK&datatype=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function for pulling implied volatility from option slam for single ticker\n",
    "'''\n",
    "\n",
    "def optionslam_scrape(ticker):\n",
    "    site = 'https://www.optionslam.com/earnings/stocks/' + ticker\n",
    "    res = requests.get(site)\n",
    "    soup = bs(requests.get(site).text, \"lxml\")\n",
    "    soup = soup.prettify()\n",
    "    earnings_dict = {'Ticker': ticker}\n",
    "    \n",
    "    # Check if there's weekly options\n",
    "    curr7_implied = \"Current 7 Day Implied Movement:\"\n",
    "    implied_move_weekly = \"Implied Move Weekly:\"\n",
    "    nextearnings = \"Next Earnings Date:\"\n",
    "    if curr7_implied not in soup:\n",
    "        return 'No Weeklies'\n",
    "    \n",
    "    # Parsing if weekly options exist\n",
    "    # Next earnings date and before or after\n",
    "    earnings_start_string = \"Next Earnings Date:\"\n",
    "    earnings_end_string = '</font>'\n",
    "    raw_earnings_string = (soup.split(earnings_start_string))[1].split(earnings_end_string)[0].replace('\\n','').strip()\n",
    "    \n",
    "    try:\n",
    "        earnings_date = str((raw_earnings_string.split('<b>'))[1].split('<font size=\"-1\">')).split(\"'\")[1].strip()\n",
    "    except:\n",
    "        return 'Error Parsing'\n",
    "    \n",
    "    earnings_time = str(raw_earnings_string[-2:].strip()).strip()\n",
    "    \n",
    "    earnings_dict['Date'] = earnings_date\n",
    "    earnings_dict['Earnings Time'] = earnings_time\n",
    "    \n",
    "    # Parsing 7 day implied move if weekly option exists\n",
    "    ending_string = '<font size=\"-2\">'\n",
    "    curr_7 = (soup.split(curr7_implied))[1].split(ending_string)[0].replace('\\n','').strip(\"\").split(\"<td>\")[-1].strip()\n",
    "    earnings_dict['Current 7 Day Implied'] = curr_7\n",
    "    \n",
    "    # Parsing Weekly Implied move if weekly option exists\n",
    "    if implied_move_weekly in soup:\n",
    "        weekly_implied = (soup.split(implied_move_weekly))[1].split(ending_string)[0].replace('\\n','').strip(\"\").split(\"<td>\")[-1].strip()\n",
    "    else:\n",
    "        weekly_implied = ''\n",
    "    earnings_dict[\"Implied Move Weekly\"] = weekly_implied\n",
    "    \n",
    "    return earnings_dict\n",
    "\n",
    "# Looping through the soup lxml text table format\n",
    "# and splitting each row as a individual string\n",
    "# and parsing string to retrieve the date,\n",
    "# open, and close information.\n",
    "\n",
    "def yahoo_table_parse(raw_html_table):\n",
    "    tickers = []\n",
    "    call_times = []\n",
    "    implied_7_day = []\n",
    "    implied_weekly = []\n",
    "    eps = []\n",
    "    i = 1\n",
    "    end_row = 10\n",
    "    for row in raw_html_table.find_all('tr'):\n",
    "        # Individual row stores current row item and delimits on '\\n'\n",
    "        individual_row = str(row).split('\\n')\n",
    "        row_items = individual_row[0].split('</span>')[:3]\n",
    "\n",
    "        if i == 1:\n",
    "            i += 1\n",
    "            continue\n",
    "        tick = row_items[0].split('data-symbol=\"')[1].split('\"')[0]\n",
    "        os_check = optionslam_scrape(tick)\n",
    "\n",
    "        if type(os_check) == str:\n",
    "            continue\n",
    "        else:\n",
    "            tickers.append(tick)\n",
    "            call_times.append(row_items[0].split('data-symbol=\"')[1].split('\"')[-1].replace('>',''))\n",
    "            eps.append(row_items[1].split('</td>')[1].split('>')[1])\n",
    "            implied_7_day.append(os_check['Current 7 Day Implied'].replace('%',''))\n",
    "            implied_weekly.append(os_check['Implied Move Weekly'].replace('%',''))\n",
    "\n",
    "\n",
    "    return pd.DataFrame({'Tickers': tickers, 'Call Times': call_times, 'EPS': eps,\n",
    "                         'Current 7 Day Implied': implied_7_day,\n",
    "                         'Implied Move Weekly': implied_weekly})\n",
    "\n",
    "\n",
    "def yahoo_earnings(date):\n",
    "    # Yahoo Earnings Calendar Check\n",
    "\n",
    "    today = date.strftime('%Y-%m-%d')\n",
    "    tables = []\n",
    "    for i in range(6):\n",
    "        yahoo_url = 'https://finance.yahoo.com/calendar/earnings?day=' + today + '&offset={}&size=100'.format(int(i*100))\n",
    "        res = requests.get(yahoo_url)\n",
    "        soup = bs(requests.get(yahoo_url).text, \"lxml\")\n",
    "\n",
    "        try:\n",
    "            table = soup.find_all('table')[0]\n",
    "            tables.append(yahoo_table_parse(table))\n",
    "        except:\n",
    "            print('No Table')\n",
    "\n",
    "    return pd.concat(tables,axis = 0, ignore_index = True)\n",
    "\n",
    "'''\n",
    "Function for getting all relevant earnings for a given starting week (Monday in dt.datetime(YYYY, m, d) format)\n",
    "Returns a dataframe with the earnings names, implied move, price, and earnings times.\n",
    "'''\n",
    "def weekly_earnings_check(start_datetime, days_forward):\n",
    "\n",
    "    start_date = start_datetime\n",
    "\n",
    "    weekly_earnings = []\n",
    "    while start_date.weekday() < days_forward:\n",
    "        try:\n",
    "            temp_earnings = yahoo_earnings(start_date)\n",
    "            temp_earnings['Earnings Date'] = start_date\n",
    "            temp_earnings['Last Close'] = 0\n",
    "            for idx, row in temp_earnings.iterrows():\n",
    "                temp_earnings.loc[idx, 'Last Close'] = ts.get_daily(row['Tickers'])[0].tail(1)['close'][0]\n",
    "            weekly_earnings.append(temp_earnings)\n",
    "            start_date = start_date + dt.timedelta(1)\n",
    "        except:\n",
    "            start_date = start_date + dt.timedelta(1)\n",
    "\n",
    "    earnings_df = pd.concat(weekly_earnings,axis = 0, ignore_index = True)\n",
    "    earnings_df = earnings_df[earnings_df['Last Close'] >= 30]\n",
    "    earnings_df['Implied Move Weekly'] = pd.to_numeric(earnings_df['Implied Move Weekly'])\n",
    "    #earnings_df = earnings_df[earnings_df['Call Times'] != '-']\n",
    "    earnings_df['Lower Bound'] = np.round(earnings_df['Last Close']*(1 - earnings_df['Implied Move Weekly']/100),2)\n",
    "    earnings_df = earnings_df.sort_values(['Earnings Date','Call Times'])\n",
    "    \n",
    "    return earnings_df\n",
    "    \n",
    "def fundamentals(ticker):\n",
    "    \n",
    "    site = 'https://finance.yahoo.com/quote/{0}?p={0}'.format(ticker)\n",
    "\n",
    "    res = requests.get(site)\n",
    "    soup = bs(res.text, 'lxml')\n",
    "    table = soup.find_all('table')[1]\n",
    "    sum_dict = {}\n",
    "\n",
    "    # Looping through the soup lxml text table format\n",
    "    # and splitting each row as a individual string\n",
    "    # and parsing string to retrieve the date,\n",
    "    # open, and close information.\n",
    "\n",
    "\n",
    "    for row in table.find_all('tr'):\n",
    "        # Individual row stores current row item and delimits on '\\n'\n",
    "        individual_row = str(row).split('\\n')[0]\n",
    "\n",
    "        # row_items is parsed string for each current row where each\n",
    "        # item in list is the date, open, high, low, close, and volume\n",
    "        row_items = individual_row.split('<span data-reactid=')[1].split('\"><!-- react-text: ')\n",
    "        \n",
    "        if len(row_items) > 1:\n",
    "            sum_item = row_items[0].split('>')[1].split('<')[0]\n",
    "            sum_value = row_items[1].split('-->')[1].split('<')[0]\n",
    "        elif 'YIELD' in row_items[0]:\n",
    "            try:\n",
    "                temp_val = row_items[0].split('-value\">')[1].split(\"</td>\")[0]\n",
    "                div_amount = float(temp_val.split(' ')[0])\n",
    "                div_yield = float(temp_val.split(' ')[1].replace('(','').replace(')','').replace('%',''))\n",
    "\n",
    "                sum_dict['Div'] = div_amount\n",
    "                sum_dict['Yield'] = div_yield\n",
    "            except:\n",
    "                sum_dict['Div'] = np.nan\n",
    "                sum_dict['Yield'] = np.nan\n",
    "        elif 'Market Cap' in row_items[0]:\n",
    "            sum_item = 'Market Cap (B)'\n",
    "            mkt_cap = row_items[0].split('data-reactid=\"')[-1].split('>')[1].split('<')[0]\n",
    "            mkt_cap_amount = float(mkt_cap[:-1])\n",
    "            if mkt_cap[-1] == 'M':\n",
    "                sum_value = mkt_cap_amount/1000\n",
    "            else:\n",
    "                sum_value = mkt_cap_amount\n",
    "        else:\n",
    "            sum_item = row_items[0].split('>')[1].split('<')[0]\n",
    "            sum_value = row_items[0].split('data-reactid=\"')[-1].split('>')[1].split('<')[0]\n",
    "        \n",
    "        sum_dict[sum_item] = sum_value\n",
    "    \n",
    "    sum_dict['Days Since Last Earnings'] = (dt.datetime.today().date() - \n",
    "                                            stock_earnings(ticker)['Earnings Dates'][1].date()).days\n",
    "\n",
    "    return pd.DataFrame(sum_dict, index = [ticker])\n",
    "\n",
    "# Function to return fundametal data of a ticker list\n",
    "def get_fundas(ticker_lst):\n",
    "    fund_lst = []\n",
    "    for tick in ticker_lst:\n",
    "        try:\n",
    "            fund_lst.append(fundamentals(tick))\n",
    "        except:\n",
    "            continue\n",
    "    return pd.concat(fund_lst,axis = 0)\n",
    "\n",
    "# Function historical data from alpha advantage\n",
    "def historical_data(ticker, day_number = 252, rolling_window = 20, outsize = 'full'):\n",
    "    alphavantage_link = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={0}&apikey=5HZEUI5AFJB06BUK&datatype=csv&outputsize={1}'.format(ticker, outsize)\n",
    "    stockframe = pd.read_csv(alphavantage_link, index_col = 0).sort_index()[['open', 'close']]\n",
    "    stockframe['daily_ret'] = np.log(stockframe['close']/stockframe['close'].shift(1))\n",
    "    stockframe['intra_ret'] = np.log(stockframe['close']/stockframe['open'])\n",
    "    stockframe['ovrnt_ret'] = np.log(stockframe['open']/stockframe['close'].shift(1))\n",
    "    stockframe['daily_vol'] = stockframe.daily_ret.rolling(window=rolling_window,center=False).std()\n",
    "    stockframe['intra_vol'] = stockframe.intra_ret.rolling(window=rolling_window,center=False).std()\n",
    "    stockframe['ovrnt_vol'] = stockframe.ovrnt_ret.rolling(window=rolling_window,center=False).std()\n",
    "    stockframe['daily_ann'] = stockframe.daily_vol*np.sqrt(252)\n",
    "    stockframe['intra_ann'] = stockframe.intra_vol*np.sqrt((24/6.5)*252)\n",
    "    stockframe['ovrnt_ann'] = stockframe.ovrnt_vol*np.sqrt((24/17.5)*252)\n",
    "    stockframe['oc_diff'] = stockframe.close - stockframe.open\n",
    "    stockframe['daily_dollar_vol'] = stockframe.daily_vol*stockframe.close.shift(1)\n",
    "    stockframe['daily_dollar_std'] = np.abs(stockframe.oc_diff/stockframe.daily_dollar_vol)\n",
    "\n",
    "    return stockframe.tail(day_number)\n",
    "\n",
    "# Function for building a dataframe of volatilities\n",
    "# Daily, Intraday, Overnight\n",
    "def current_volatility(ticker_list, roll = 20):\n",
    "    \n",
    "    rows = []\n",
    "    failed_tickers = []\n",
    "    \n",
    "    def failed_check(failed_lst,rows):\n",
    "        if len(failed_lst) == 0:\n",
    "            return failed_lst, rows\n",
    "        else:\n",
    "            new_lst = []\n",
    "            new_rows = rows\n",
    "            for tick in failed_lst:\n",
    "                try: \n",
    "                    curr_vol = historical_data(tick, outsize = 'compact').tail(1)[['daily_ann','intra_ann','ovrnt_ann','close',\n",
    "                                                                                   'daily_dollar_vol']]\n",
    "                    curr_vol.index.name = 'Tickers'\n",
    "                    curr_vol.index = [tick]\n",
    "                    new_rows.append(curr_vol)\n",
    "                except:\n",
    "                    new_lst.append(tick)\n",
    "            return failed_check(new_lst, rows)\n",
    "\n",
    "    for tick in ticker_list:\n",
    "        try: \n",
    "            curr_vol = historical_data(tick, outsize = 'compact').tail(1)[['daily_ann','intra_ann','ovrnt_ann','close',\n",
    "                                                                           'daily_dollar_vol']]\n",
    "            curr_vol.index.name = 'Tickers'\n",
    "            curr_vol.index = [tick]\n",
    "            rows.append(curr_vol)\n",
    "        except:\n",
    "            failed_tickers.append(tick)\n",
    "            \n",
    "    failed_lst, rows = failed_check(failed_tickers, rows)\n",
    "        \n",
    "    return pd.concat(rows, axis = 0)\n",
    "\n",
    "def all_options(ticker):\n",
    "    tape = Options(ticker, 'yahoo')\n",
    "    data = tape.get_all_data().reset_index()\n",
    "    \n",
    "    data['Moneyness'] = np.abs(data['Strike'] - data['Underlying_Price'])/data['Underlying_Price']\n",
    "    \n",
    "    data['DTE'] = (data['Expiry'] - dt.datetime.today()).dt.days\n",
    "    data = data[['Strike', 'DTE', 'Type', 'IV', 'Vol','Open_Int', 'Moneyness', 'Root', 'Underlying_Price',\n",
    "                 'Last','Bid','Ask']]\n",
    "    data['Mid'] = (data['Ask'] - data['Bid'])/2 + data['Bid']\n",
    "    \n",
    "    year = 365\n",
    "    strikes = data['Strike'].values\n",
    "    time_to_expirations = data['DTE'].values\n",
    "    ivs = data['IV'].values\n",
    "    underlying = data['Underlying_Price'].values[0]\n",
    "    types = data['Type'].values\n",
    "\n",
    "    # Make sure nothing thows up\n",
    "    assert len(strikes) == len(time_to_expirations)\n",
    "\n",
    "    sigmas = data['IV']\n",
    "    deltas = []\n",
    "    gammas = []\n",
    "    thetas = []\n",
    "    vegas = []\n",
    "    for sigma, strike, time_to_expiration, flag in zip(sigmas, strikes, time_to_expirations, types):\n",
    "\n",
    "        # Constants\n",
    "        S = underlying\n",
    "        K = strike\n",
    "        t = time_to_expiration/float(year)\n",
    "        r = 0.005 / 100\n",
    "        q = 0 / 100\n",
    "\n",
    "        try:\n",
    "            delta = py_vollib.black_scholes_merton.greeks.analytical.delta(flag[0], S, K, t, r, sigma, q)\n",
    "            deltas.append(delta)\n",
    "        except:\n",
    "            delta = 0.0\n",
    "            deltas.append(delta)\n",
    "\n",
    "        try:\n",
    "            gamma = py_vollib.black_scholes_merton.greeks.analytical.gamma(flag[0], S, K, t, r, sigma, q)\n",
    "            gammas.append(gamma)\n",
    "        except:\n",
    "            gamma = 0.0\n",
    "            gammas.append(gamma)\n",
    "\n",
    "        try:\n",
    "            theta = py_vollib.black_scholes_merton.greeks.analytical.theta(flag[0], S, K, t, r, sigma, q)\n",
    "            thetas.append(theta)\n",
    "        except:\n",
    "            theta = 0.0\n",
    "            thetas.append(theta)\n",
    "\n",
    "        try:\n",
    "            vega = py_vollib.black_scholes_merton.greeks.analytical.vega(flag[0], S, K, t, r, sigma, q)\n",
    "            vegas.append(vega)\n",
    "        except:\n",
    "            vega = 0.0\n",
    "            vegas.append(vega)\n",
    "\n",
    "    data['Delta'] = deltas\n",
    "    data['Gamma'] = gammas\n",
    "    data['Theta'] = thetas\n",
    "    data['Vega'] = vegas\n",
    "\n",
    "    return data.dropna().reset_index()[data.columns]\n",
    "\n",
    "\n",
    "def earnings_condor(tick, max_gap, dte_thresh, money_thresh):\n",
    "    chain = all_options(tick)\n",
    "    chain = chain[chain['DTE'] <= dte_thresh]\n",
    "    chain = chain.reset_index()[chain.columns]\n",
    "    chain = chain[chain['Moneyness'] <= money_thresh]\n",
    "    chain_puts = chain[(chain['Type'] == 'put') & (chain['Strike'] < chain['Underlying_Price'].values[0])]\n",
    "    chain_calls = chain[(chain['Type'] == 'call') & (chain['Strike'] > chain['Underlying_Price'].values[0])]\n",
    "\n",
    "\n",
    "    put_spread_prem = []\n",
    "    put_spread_delta = []\n",
    "    put_spread_gamma = []\n",
    "    put_spread_vega = []\n",
    "    put_spread_theta = []\n",
    "    put_spread_short_strike = []\n",
    "    put_spread_long_strike = []\n",
    "    put_spread_max_loss = []\n",
    "    for idx, row in chain_puts.sort_values('Strike', ascending = False).iterrows():\n",
    "        curr_short_strike = row.Strike\n",
    "        curr_short_prem = row.Bid\n",
    "        curr_short_delta = row.Delta\n",
    "        curr_short_gamma = row.Gamma\n",
    "        curr_short_vega = row.Vega\n",
    "        curr_short_theta = row.Theta\n",
    "\n",
    "        temp_longs = chain_puts[(chain_puts['Strike'] < curr_short_strike) &\n",
    "                                (chain_puts['Strike'] >= curr_short_strike - max_gap)]\n",
    "\n",
    "        for temp_idx, temp_row in temp_longs.iterrows():\n",
    "            curr_long_strike = temp_row.Strike\n",
    "            curr_long_prem = temp_row.Ask\n",
    "            curr_long_delta = temp_row.Delta\n",
    "            curr_long_gamma = temp_row.Gamma\n",
    "            curr_long_vega = temp_row.Vega\n",
    "            curr_long_theta = temp_row.Theta\n",
    "\n",
    "            curr_spread_prem = curr_short_prem - curr_long_prem\n",
    "            curr_spread_delta = -curr_short_delta + curr_long_delta\n",
    "            curr_spread_gamma = -curr_short_gamma + curr_long_gamma\n",
    "            curr_spread_vega = -curr_short_vega + curr_long_vega\n",
    "            curr_spread_theta = -curr_short_theta + curr_long_theta\n",
    "            curr_spread_maxloss = (curr_short_strike - curr_long_strike - curr_spread_prem)*100\n",
    "\n",
    "            put_spread_prem.append(curr_spread_prem)\n",
    "            put_spread_delta.append(curr_spread_delta)\n",
    "            put_spread_gamma.append(curr_spread_gamma)\n",
    "            put_spread_vega.append(curr_spread_vega)\n",
    "            put_spread_theta.append(curr_spread_theta)\n",
    "            put_spread_short_strike.append(curr_short_strike)\n",
    "            put_spread_long_strike.append(curr_long_strike)\n",
    "            put_spread_max_loss.append(curr_spread_maxloss)\n",
    "\n",
    "    put_spreads_df = pd.DataFrame(OrderedDict({'put Combo': range(len(put_spread_prem)),\n",
    "                                               'Short Put Strike': put_spread_short_strike,\n",
    "                                               'Long Put Strike': put_spread_long_strike,\n",
    "                                               'put Spread Premium': put_spread_prem,\n",
    "                                               'put Spread Maxloss': put_spread_max_loss,\n",
    "                                               'put Spread Delta': put_spread_delta,\n",
    "                                               'put Spread Gamma': put_spread_gamma,\n",
    "                                               'put Spread Vega': put_spread_vega,\n",
    "                                               'put Spread Theta': put_spread_theta}),\n",
    "                                  index = range(len(put_spread_prem)))\n",
    "\n",
    "    call_spread_prem = []\n",
    "    call_spread_delta = []\n",
    "    call_spread_gamma = []\n",
    "    call_spread_vega = []\n",
    "    call_spread_theta = []\n",
    "    call_spread_short_strike = []\n",
    "    call_spread_long_strike = []\n",
    "    call_spread_max_loss = []\n",
    "    for idx, row in chain_calls.sort_values('Strike', ascending = True).iterrows():\n",
    "        curr_short_strike = row.Strike\n",
    "        curr_short_prem = row.Bid\n",
    "        curr_short_delta = row.Delta\n",
    "        curr_short_gamma = row.Gamma\n",
    "        curr_short_vega = row.Vega\n",
    "        curr_short_theta = row.Theta\n",
    "\n",
    "        temp_longs = chain_calls[(chain_calls['Strike'] > curr_short_strike) &\n",
    "                                (chain_calls['Strike'] <= curr_short_strike + max_gap)]\n",
    "\n",
    "        for temp_idx, temp_row in temp_longs.iterrows():\n",
    "            curr_long_strike = temp_row.Strike\n",
    "            curr_long_prem = temp_row.Ask\n",
    "            curr_long_delta = temp_row.Delta\n",
    "            curr_long_gamma = temp_row.Gamma\n",
    "            curr_long_vega = temp_row.Vega\n",
    "            curr_long_theta = temp_row.Theta\n",
    "\n",
    "            curr_spread_prem = curr_short_prem - curr_long_prem\n",
    "            curr_spread_delta = -curr_short_delta + curr_long_delta\n",
    "            curr_spread_gamma = -curr_short_gamma + curr_long_gamma\n",
    "            curr_spread_vega = -curr_short_vega + curr_long_vega\n",
    "            curr_spread_theta = -curr_short_theta + curr_long_theta\n",
    "            curr_spread_maxloss = -(curr_short_strike - curr_long_strike + curr_spread_prem)*100\n",
    "\n",
    "            call_spread_prem.append(curr_spread_prem)\n",
    "            call_spread_delta.append(curr_spread_delta)\n",
    "            call_spread_gamma.append(curr_spread_gamma)\n",
    "            call_spread_vega.append(curr_spread_vega)\n",
    "            call_spread_theta.append(curr_spread_theta)\n",
    "            call_spread_short_strike.append(curr_short_strike)\n",
    "            call_spread_long_strike.append(curr_long_strike)\n",
    "            call_spread_max_loss.append(curr_spread_maxloss)\n",
    "\n",
    "    call_spreads_df = pd.DataFrame(OrderedDict({'call Combo': range(len(call_spread_prem)),\n",
    "                                               'Short call Strike': call_spread_short_strike,\n",
    "                                               'Long call Strike': call_spread_long_strike,\n",
    "                                               'call Spread Premium': call_spread_prem,\n",
    "                                               'call Spread Maxloss': call_spread_max_loss,\n",
    "                                               'call Spread Delta': call_spread_delta,\n",
    "                                               'call Spread Gamma': call_spread_gamma,\n",
    "                                               'call Spread Vega': call_spread_vega,\n",
    "                                               'call Spread Theta': call_spread_theta}),\n",
    "                                  index = range(len(call_spread_prem)))\n",
    "\n",
    "    put_combos = []\n",
    "    call_combos = []\n",
    "    condor_prems = []\n",
    "    condor_maxloss = []\n",
    "    condor_delta = []\n",
    "    condor_gamma = []\n",
    "    condor_vega = []\n",
    "    condor_theta = []\n",
    "    put_short = []\n",
    "    put_long = []\n",
    "    call_short = []\n",
    "    call_long = []\n",
    "\n",
    "    for idxc, rowc in call_spreads_df.iterrows():\n",
    "        for idxp, rowp in put_spreads_df.iterrows():\n",
    "            p_s = put_spreads_df[put_spreads_df['put Combo'] == rowp['put Combo']]['Short Put Strike'].values[0]\n",
    "            p_l = put_spreads_df[put_spreads_df['put Combo'] == rowp['put Combo']]['Long Put Strike'].values[0]\n",
    "            c_s = call_spreads_df[call_spreads_df['call Combo'] == rowc['call Combo']]['Short call Strike'].values[0]\n",
    "            c_l = call_spreads_df[call_spreads_df['call Combo'] == rowc['call Combo']]['Long call Strike'].values[0]\n",
    "\n",
    "            put_short.append(p_s)\n",
    "            put_long.append(p_l)\n",
    "            call_short.append(c_s)\n",
    "            call_long.append(c_l)\n",
    "\n",
    "            curr_prem = round(rowp['put Spread Premium'] + rowc['call Spread Premium'],2)\n",
    "\n",
    "            condor_prems.append(curr_prem)\n",
    "            condor_maxloss.append(100*(max(p_s - p_l, c_l - c_s) - curr_prem))\n",
    "            condor_delta.append(rowp['put Spread Delta'] + rowc['call Spread Delta'])\n",
    "            condor_gamma.append(rowp['put Spread Gamma'] + rowc['call Spread Gamma'])\n",
    "            condor_vega.append(rowp['put Spread Vega'] + rowc['call Spread Vega'])\n",
    "            condor_theta.append(rowp['put Spread Theta'] + rowc['call Spread Theta'])\n",
    "\n",
    "    condors_df = pd.DataFrame(OrderedDict({'P Short Strike': put_short,\n",
    "                                           'P Long Strike': put_long,\n",
    "                                           'C Short Strike': call_short,\n",
    "                                           'C Long Strike': call_long,\n",
    "                                           'Premium': condor_prems,\n",
    "                                           'Maxloss': condor_maxloss,\n",
    "                                           'Delta': condor_delta,\n",
    "                                           'Gamma': condor_gamma,\n",
    "                                           'Vega': condor_vega,\n",
    "                                           'Theta': condor_theta}),\n",
    "                                  index = range(len(condor_prems)))\n",
    "    condors_df['RiskRewardRatio'] = round((100*condors_df['Premium'])/condors_df['Maxloss'],2)\n",
    "    \n",
    "    return condors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "tick = 'CSX'\n",
    "max_gap = 2\n",
    "dte_thresh = 5\n",
    "money_thresh = 0.1\n",
    "\n",
    "csx = earnings_condor(tick, max_gap, dte_thresh, money_thresh)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.92\n",
       "1       0.80\n",
       "2       0.63\n",
       "3       0.45\n",
       "4       0.79\n",
       "5       0.66\n",
       "6       0.54\n",
       "7       0.37\n",
       "8       0.69\n",
       "9       0.59\n",
       "10      0.46\n",
       "11      0.34\n",
       "12      0.66\n",
       "13      0.55\n",
       "14      0.45\n",
       "15      0.32\n",
       "16      0.56\n",
       "17      0.52\n",
       "18      0.41\n",
       "19      0.31\n",
       "20      0.50\n",
       "21      0.45\n",
       "22      0.41\n",
       "23      0.30\n",
       "24      0.44\n",
       "25      0.38\n",
       "26      0.33\n",
       "27      0.29\n",
       "28      0.37\n",
       "29      0.35\n",
       "        ... \n",
       "1734    0.29\n",
       "1735    0.18\n",
       "1736    0.08\n",
       "1737   -0.05\n",
       "1738    0.19\n",
       "1739    0.15\n",
       "1740    0.04\n",
       "1741   -0.06\n",
       "1742    0.13\n",
       "1743    0.08\n",
       "1744    0.04\n",
       "1745   -0.07\n",
       "1746    0.07\n",
       "1747    0.01\n",
       "1748   -0.04\n",
       "1749   -0.08\n",
       "1750    0.00\n",
       "1751   -0.02\n",
       "1752   -0.08\n",
       "1753   -0.13\n",
       "1754   -0.01\n",
       "1755   -0.04\n",
       "1756   -0.06\n",
       "1757   -0.12\n",
       "1758   -0.08\n",
       "1759   -0.11\n",
       "1760   -0.13\n",
       "1761   -0.12\n",
       "1762   -0.15\n",
       "1763   -0.15\n",
       "Name: Premium, Length: 1764, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(csx['Premium'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
