{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note to import from .py files, must follow structure\n",
    "# from <.py filename excluding '.py'> import <class name>\n",
    "# Optionslam creds: aspringfastlaner Options2018\n",
    "\n",
    "# Importing necessary models\n",
    "import smtplib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas.stats.moments as st\n",
    "from pandas import ExcelWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as dates\n",
    "# import matplotlib.ticker as ticker\n",
    "from lxml import html\n",
    "import requests\n",
    "import webbrowser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "import csv\n",
    "import sched, time\n",
    "import pandas_datareader as datareader\n",
    "from pandas_datareader.data import Options\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "ts = TimeSeries(key='5HZEUI5AFJB06BUK',output_format='pandas')\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request as urlreq\n",
    "\n",
    "from pandas_datareader.data import Options\n",
    "from py_vollib.black_scholes_merton.implied_volatility import *\n",
    "\n",
    "'''\n",
    "Calculate the Black-Scholes implied volatility.\n",
    "\n",
    "Parameters:\t\n",
    "price (float) – the Black-Scholes option price\n",
    "S (float) – underlying asset price\n",
    "K (float) – strike price\n",
    "t (float) – time to expiration in years\n",
    "r (float) – risk-free interest rate\n",
    "flag (str) – ‘c’ or ‘p’ for call or put.\n",
    ">>> S = 100\n",
    ">>> K = 100\n",
    ">>> sigma = .2\n",
    ">>> r = .01\n",
    ">>> flag = 'c'\n",
    ">>> t = .5\n",
    ">>> price = black_scholes(flag, S, K, t, r, sigma)\n",
    ">>> iv = implied_volatility(price, S, K, t, r, flag)\n",
    "'''\n",
    "%matplotlib inline\n",
    "\n",
    "def write_excel(filename, sheetnames, df_list):\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "    writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n",
    "    for i, df in enumerate(df_list):\n",
    "        \n",
    "        df.to_excel(writer, sheet_name = sheetnames[i])\n",
    "\n",
    "    # Close the Pandas Excel writer and output the Excel file.\n",
    "    writer.save()\n",
    "    return\n",
    "\n",
    "# Alpha Vantage API Key\n",
    "# 5HZEUI5AFJB06BUK\n",
    "\n",
    "# ts = TimeSeries(key='5HZEUI5AFJB06BUK', output_format='pandas')\n",
    "# data, meta_data = ts.get_intraday(symbol='MSFT',interval='1min', outputsize='full')\n",
    "# data['close'].plot()\n",
    "# plt.title('Intraday Times Series for the MSFT stock (1 min)')\n",
    "# For intraday\n",
    "# https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=MSFT&interval=1min&apikey=d5HZEUI5AFJB06BUK&datatype=csv\n",
    "\n",
    "# For daily\n",
    "# https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=MSFT&apikey=5HZEUI5AFJB06BUK&datatype=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Pulling S&P 500 Names\n",
    "'''\n",
    "\n",
    "def pull_sp500_list():\n",
    "    site = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "    res = requests.get(site)\n",
    "    soup = bs(res.text, 'lxml')\n",
    "    table = soup.find_all('table')[0]\n",
    "\n",
    "    tickers = []\n",
    "    names = []\n",
    "    gics = []\n",
    "\n",
    "    # Looping through the soup lxml text table format\n",
    "    # and splitting each row as a individual string\n",
    "    # and parsing string to retrieve the date,\n",
    "    # open, and close information.\n",
    "    i = 1\n",
    "    for row in table.find_all('tr'):\n",
    "        if i == 1:\n",
    "            i += 1\n",
    "            continue\n",
    "        # Individual row stores current row item and delimits on '\\n'\n",
    "        individual_row = str(row).split('\\n')\n",
    "        # row_items is parsed string for each current row where each\n",
    "        ticker = individual_row[1].split('\">')[-1].split('<')[0]\n",
    "        tickers.append(ticker)\n",
    "        name = individual_row[2].split('\">')[-1].split('<')[0]\n",
    "        names.append(name)\n",
    "        gic = individual_row[4].split('>')[1].split('<')[0]\n",
    "        gics.append(gic)\n",
    "\n",
    "    sp500 = pd.DataFrame({'Name': names, 'GIC': gics}, index = tickers)\n",
    "    sp500.index.name = 'Tickers'\n",
    "    return sp500\n",
    "\n",
    "#nasdaq = pd.read_csv('http://www.nasdaq.com/screening/companies-by-industry.aspx?exchange=NASDAQ&render=download', index_col = 0)[['Name','LastSale','IPOyear','Sector']]\n",
    "#nyse = pd.read_csv('http://www.nasdaq.com/screening/companies-by-industry.aspx?exchange=NYSE&render=download', index_col = 0)[['Name','LastSale','IPOyear','Sector']]\n",
    "#us_stocks = pd.concat([nyse,nasdaq], axis = 0).drop_duplicates()\n",
    "#us_stocks = us_stocks[us_stocks['LastSale'] != 'n/a']\n",
    "#us_fundamentals = get_fundas([string.replace(' ','') for string in us_stocks.index.tolist()])\n",
    "us_stocks = pd.read_csv('us_stocks.csv', index_col = 0)\n",
    "\n",
    "active_stocks = pd.read_csv('active_names.csv', index_col = 0).dropna()\n",
    "active_etfs = pd.read_csv('active_etfs.csv', index_col = 0).dropna()\n",
    "highest_ivs = pd.read_csv('highest_iv.csv', index_col = 0).dropna()\n",
    "\n",
    "# filtered_names = pd.read_csv('filtered_names.csv', index_col = 0).join(us_stocks, how = 'inner')[us_stocks.columns]\n",
    "# filtered_names['Market Cap'] = filtered_names['Market Cap'].astype(str).str[:-1]\n",
    "# filtered_names['Market Cap'] = filtered_names['Market Cap'].astype(float)\n",
    "# filtered_names = filtered_names.sort_values(['Market Cap'], ascending = False)\n",
    "\n",
    "watchlist = ['NVDA', 'FB', 'AMZN', 'NFLX', 'GOOGL', 'GOOG',\n",
    "             'TSLA', 'EA', 'ATVI', 'APPL', 'MSFT', 'INTC',\n",
    "             'V', 'CSCO', 'VZ', 'T', 'MA', 'ORCL', 'IBM',\n",
    "             'ADBE', 'TXN', 'AVGO', 'PYPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function for pulling implied volatility from option slam for single ticker\n",
    "'''\n",
    "\n",
    "def optionslam_scrape(ticker):\n",
    "    site = 'https://www.optionslam.com/earnings/stocks/' + ticker\n",
    "    res = requests.get(site)\n",
    "    soup = bs(requests.get(site).text, \"lxml\")\n",
    "    soup = soup.prettify()\n",
    "    earnings_dict = {'Ticker': ticker}\n",
    "    \n",
    "    # Check if there's weekly options\n",
    "    curr7_implied = \"Current 7 Day Implied Movement:\"\n",
    "    implied_move_weekly = \"Implied Move Weekly:\"\n",
    "    nextearnings = \"Next Earnings Date:\"\n",
    "    if curr7_implied not in soup:\n",
    "        return 'No Weeklies'\n",
    "    \n",
    "    # Parsing if weekly options exist\n",
    "    # Next earnings date and before or after\n",
    "    earnings_start_string = \"Next Earnings Date:\"\n",
    "    earnings_end_string = '</font>'\n",
    "    raw_earnings_string = (soup.split(earnings_start_string))[1].split(earnings_end_string)[0].replace('\\n','').strip()\n",
    "    \n",
    "    try:\n",
    "        earnings_date = str((raw_earnings_string.split('<b>'))[1].split('<font size=\"-1\">')).split(\"'\")[1].strip()\n",
    "    except:\n",
    "        return 'Error Parsing'\n",
    "    \n",
    "    earnings_time = str(raw_earnings_string[-2:].strip()).strip()\n",
    "    \n",
    "    earnings_dict['Date'] = earnings_date\n",
    "    earnings_dict['Earnings Time'] = earnings_time\n",
    "    \n",
    "    # Parsing 7 day implied move if weekly option exists\n",
    "    ending_string = '<font size=\"-2\">'\n",
    "    curr_7 = (soup.split(curr7_implied))[1].split(ending_string)[0].replace('\\n','').strip(\"\").split(\"<td>\")[-1].strip()\n",
    "    earnings_dict['Current 7 Day Implied'] = curr_7\n",
    "    \n",
    "    # Parsing Weekly Implied move if weekly option exists\n",
    "    if implied_move_weekly in soup:\n",
    "        weekly_implied = (soup.split(implied_move_weekly))[1].split(ending_string)[0].replace('\\n','').strip(\"\").split(\"<td>\")[-1].strip()\n",
    "    else:\n",
    "        weekly_implied = ''\n",
    "    earnings_dict[\"Implied Move Weekly\"] = weekly_implied\n",
    "    \n",
    "    return earnings_dict\n",
    "\n",
    "# Looping through the soup lxml text table format\n",
    "# and splitting each row as a individual string\n",
    "# and parsing string to retrieve the date,\n",
    "# open, and close information.\n",
    "\n",
    "def yahoo_table_parse(raw_html_table):\n",
    "    tickers = []\n",
    "    call_times = []\n",
    "    implied_7_day = []\n",
    "    implied_weekly = []\n",
    "    eps = []\n",
    "    i = 1\n",
    "    end_row = 10\n",
    "    for row in raw_html_table.find_all('tr'):\n",
    "        # Individual row stores current row item and delimits on '\\n'\n",
    "        individual_row = str(row).split('\\n')\n",
    "        row_items = individual_row[0].split('</span>')[:3]\n",
    "\n",
    "        if i == 1:\n",
    "            i += 1\n",
    "            continue\n",
    "        tick = row_items[0].split('data-symbol=\"')[1].split('\"')[0]\n",
    "        os_check = optionslam_scrape(tick)\n",
    "\n",
    "        if type(os_check) == str:\n",
    "            continue\n",
    "        else:\n",
    "            tickers.append(tick)\n",
    "            call_times.append(row_items[0].split('data-symbol=\"')[1].split('\"')[-1].replace('>',''))\n",
    "            eps.append(row_items[1].split('</td>')[1].split('>')[1])\n",
    "            implied_7_day.append(os_check['Current 7 Day Implied'].replace('%',''))\n",
    "            implied_weekly.append(os_check['Implied Move Weekly'].replace('%',''))\n",
    "\n",
    "\n",
    "    return pd.DataFrame({'Tickers': tickers, 'Call Times': call_times, 'EPS': eps,\n",
    "                         'Current 7 Day Implied': implied_7_day,\n",
    "                         'Implied Move Weekly': implied_weekly})\n",
    "\n",
    "\n",
    "def yahoo_earnings(date):\n",
    "    # Yahoo Earnings Calendar Check\n",
    "\n",
    "    today = date.strftime('%Y-%m-%d')\n",
    "    tables = []\n",
    "    for i in range(6):\n",
    "        yahoo_url = 'https://finance.yahoo.com/calendar/earnings?day=' + today + '&offset={}&size=100'.format(int(i*100))\n",
    "        res = requests.get(yahoo_url)\n",
    "        soup = bs(requests.get(yahoo_url).text, \"lxml\")\n",
    "\n",
    "        try:\n",
    "            table = soup.find_all('table')[0]\n",
    "            tables.append(yahoo_table_parse(table))\n",
    "        except:\n",
    "            print('No Table')\n",
    "\n",
    "    return pd.concat(tables,axis = 0, ignore_index = True)\n",
    "\n",
    "'''\n",
    "Functions for pulling options data from yahoo Input is a string. The output is a dataframe of the latest\n",
    "data from yahoo finance tagged with the current date-time. Output columns are pull date-time,\n",
    "contract name, strike, last price, bid, ask volume, open interest, and IV (in decimal form).\n",
    "'''\n",
    "# Function for initial querying of yahoo data\n",
    "def yahoo_option_query(ticker, unix_date):\n",
    "    # dt.datetime.fromtimestamp(1525996800).date()\n",
    "    if unix_date == 'None':\n",
    "        yahoo_query = 'https://query1.finance.yahoo.com/v7/finance/options/{0}'.format(ticker)\n",
    "    else:\n",
    "        yahoo_query = 'https://query1.finance.yahoo.com/v7/finance/options/{0}?date={1}'.format(ticker,str(unix_date))\n",
    "        \n",
    "    response = urlreq.urlopen(yahoo_query)\n",
    "    data = json.loads(response.read().decode())['optionChain']['result'][0]\n",
    "    \n",
    "    dict_lst = []\n",
    "    for key in data.keys():\n",
    "        dict_lst.append(data[key])\n",
    "        \n",
    "    expiries = dict_lst[1]\n",
    "    strikes = dict_lst[2]\n",
    "    underlying = dict_lst[4]\n",
    "    calls = dict_lst[5][0]['calls']\n",
    "    puts = dict_lst[5][0]['puts']\n",
    "    \n",
    "    return (expiries, strikes, underlying, calls, puts)\n",
    "\n",
    "# Function for creating dataframe for options contracts for a specific maturity date\n",
    "def create_contract_df(option_dict_lst, strikes):\n",
    "    df = pd.DataFrame(columns = ['lastPrice','volume','openInterest','bid','ask','mid','impliedVolatility','expiration'],\n",
    "                      index = strikes)\n",
    "    for contract in option_dict_lst:\n",
    "        for col in df.columns:\n",
    "            if col == 'expiration':\n",
    "                df.loc[contract['strike'], col] = (dt.datetime.fromtimestamp(contract['expiration']) - dt.datetime.today()).days\n",
    "            elif col == 'mid':\n",
    "                df.loc[contract['strike'], col] = (contract['ask'] + contract['bid'])/2\n",
    "            else:\n",
    "                df.loc[contract['strike'], col] = contract[col]\n",
    "    return df.dropna()\n",
    "\n",
    "# Function for creating straddle view of options for a specific date\n",
    "def option_chain(calls, puts, strikes):\n",
    "    call_contracts = create_contract_df(calls, strikes)\n",
    "    put_contracts = create_contract_df(puts, strikes)\n",
    "    return call_contracts.join(put_contracts, how = 'inner', lsuffix='_c', rsuffix='_p')\n",
    "\n",
    "# Function for getting full option data for a specific ticker\n",
    "def pull_options(ticker):\n",
    "    initial_near_contract = yahoo_option_query(ticker, 'None')\n",
    "    \n",
    "    expiries = initial_near_contract[0]\n",
    "    options_list = [option_chain(initial_near_contract[3], initial_near_contract[4], initial_near_contract[1])]\n",
    "    \n",
    "    for expiry in expiries[1:]:\n",
    "        next_contract = yahoo_option_query(ticker, expiry)\n",
    "        options_list.append(option_chain(next_contract[3], next_contract[4], next_contract[1]))\n",
    "        \n",
    "    return pd.concat(options_list, axis = 0)\n",
    "ts.get_daily('AAPL')[0].tail(1)['close'][0]\n",
    "'''\n",
    "Function for getting all relevant earnings for a given starting week (Monday in dt.datetime(YYYY, m, d) format)\n",
    "Returns a dataframe with the earnings names, implied move, price, and earnings times.\n",
    "'''\n",
    "def weekly_earnings_check(start_datetime, days_forward):\n",
    "\n",
    "    start_date = start_datetime\n",
    "\n",
    "    weekly_earnings = []\n",
    "    while start_date.weekday() < days_forward:\n",
    "        try:\n",
    "            temp_earnings = yahoo_earnings(start_date)\n",
    "            temp_earnings['Earnings Date'] = start_date\n",
    "            temp_earnings['Last Close'] = 0\n",
    "            for idx, row in temp_earnings.iterrows():\n",
    "                temp_earnings.loc[idx, 'Last Close'] = ts.get_daily(row['Tickers'])[0].tail(1)['close'][0]\n",
    "            weekly_earnings.append(temp_earnings)\n",
    "            start_date = start_date + dt.timedelta(1)\n",
    "        except:\n",
    "            start_date = start_date + dt.timedelta(1)\n",
    "\n",
    "    earnings_df = pd.concat(weekly_earnings,axis = 0, ignore_index = True)\n",
    "    earnings_df = earnings_df[earnings_df['Last Close'] >= 30]\n",
    "    earnings_df['Implied Move Weekly'] = pd.to_numeric(earnings_df['Implied Move Weekly'])\n",
    "    #earnings_df = earnings_df[earnings_df['Call Times'] != '-']\n",
    "    earnings_df['Lower Bound'] = np.round(earnings_df['Last Close']*(1 - earnings_df['Implied Move Weekly']/100),2)\n",
    "    earnings_df = earnings_df.sort_values(['Earnings Date','Call Times'])\n",
    "    \n",
    "    return earnings_df\n",
    "\n",
    "def stock_earnings(tick):\n",
    "    yahoo_url = 'https://finance.yahoo.com/calendar/earnings/?symbol={0}'.format(tick)\n",
    "    res = requests.get(yahoo_url).text\n",
    "    res = res.split('table class')[1].split('</table>')[0].split('<tbody data-reactid=\"')[1].split('<td class=\"')\n",
    "    earnings_cols = list(filter(lambda x: 'data-col2' in x, res))\n",
    "    estimate_cols = list(filter(lambda x: 'data-col3' in x, res))\n",
    "    reported_cols = list(filter(lambda x: 'data-col4' in x, res))\n",
    "\n",
    "    earnings_days = []\n",
    "    for i in earnings_cols:\n",
    "        temp_date = i.split('\">')[2].split('</')[0]\n",
    "        temp_date = ''.join(temp_date.split(',')[:-1])\n",
    "        temp_date = dt.datetime.strptime(temp_date, '%b %d %Y')\n",
    "        earnings_days.append(temp_date)\n",
    "\n",
    "    estimate_eps = []\n",
    "    for i in estimate_cols:\n",
    "        try:\n",
    "            temp_est = float(i.split('\">')[1].split('<')[0])\n",
    "        except:\n",
    "            temp_est = np.nan\n",
    "        estimate_eps.append(temp_est)\n",
    "\n",
    "    reported_eps = []\n",
    "    for i in reported_cols:\n",
    "        try:\n",
    "            temp_est = float(i.split('\">')[1].split('<')[0])\n",
    "        except:\n",
    "            temp_est = np.nan\n",
    "        reported_eps.append(temp_est)\n",
    "\n",
    "    earnings = pd.DataFrame({'Earnings Dates': earnings_days,\n",
    "                             'EPS Estimate': estimate_eps,\n",
    "                             'EPS Reported': reported_eps})\n",
    "    return earnings\n",
    "\n",
    "def old_weekly_earnings_check(start_datetime, days_forward):\n",
    "    \n",
    "    def yahoo_earnings_raw(raw_html_table):\n",
    "        tickers = []\n",
    "        eps = []\n",
    "        i = 1\n",
    "        end_row = 10\n",
    "        for row in raw_html_table.find_all('tr'):\n",
    "            # Individual row stores current row item and delimits on '\\n'\n",
    "            individual_row = str(row).split('\\n')\n",
    "            row_items = individual_row[0].split('</span>')[:3]\n",
    "\n",
    "            if i == 1:\n",
    "                i += 1\n",
    "                continue\n",
    "            tick = row_items[0].split('data-symbol=\"')[1].split('\"')[0]\n",
    "            tickers.append(tick)\n",
    "\n",
    "\n",
    "        return pd.DataFrame({'Tickers': tickers})\n",
    "    \n",
    "    def yahoo_earnings_old(date):\n",
    "        # Yahoo Earnings Calendar Check\n",
    "\n",
    "        today = date.strftime('%Y-%m-%d')\n",
    "        tables = []\n",
    "        for i in range(6):\n",
    "            yahoo_url = 'https://finance.yahoo.com/calendar/earnings?day=' + today + '&offset={}&size=100'.format(int(i*100))\n",
    "            res = requests.get(yahoo_url)\n",
    "            soup = bs(requests.get(yahoo_url).text, \"lxml\")\n",
    "\n",
    "            try:\n",
    "                table = soup.find_all('table')[0]\n",
    "                tables.append(yahoo_earnings_raw(table))\n",
    "            except:\n",
    "                print('No Table')\n",
    "\n",
    "        return pd.concat(tables,axis = 0, ignore_index = True)\n",
    "    \n",
    "    start_date = start_datetime\n",
    "\n",
    "    weekly_earnings = []\n",
    "    while start_date.weekday() < days_forward:\n",
    "        try: \n",
    "            temp_earnings = yahoo_earnings_old(start_date)\n",
    "            temp_earnings['Earnings Date'] = start_date\n",
    "            weekly_earnings.append(temp_earnings)\n",
    "            start_date = start_date + dt.timedelta(1)\n",
    "        except:\n",
    "            start_date = start_date + dt.timedelta(1)\n",
    "            continue\n",
    "\n",
    "    earnings_df = pd.concat(weekly_earnings,axis = 0, ignore_index = True)\n",
    "    #earnings_df = earnings_df[earnings_df['Call Times'] != '-']\n",
    "    \n",
    "    return earnings_df\n",
    "\n",
    "    \n",
    "def fundamentals(ticker):\n",
    "    \n",
    "    site = 'https://finance.yahoo.com/quote/{0}?p={0}'.format(ticker)\n",
    "\n",
    "    res = requests.get(site)\n",
    "    soup = bs(res.text, 'lxml')\n",
    "    table = soup.find_all('table')[1]\n",
    "    sum_dict = {}\n",
    "\n",
    "    # Looping through the soup lxml text table format\n",
    "    # and splitting each row as a individual string\n",
    "    # and parsing string to retrieve the date,\n",
    "    # open, and close information.\n",
    "\n",
    "\n",
    "    for row in table.find_all('tr'):\n",
    "        # Individual row stores current row item and delimits on '\\n'\n",
    "        individual_row = str(row).split('\\n')[0]\n",
    "\n",
    "        # row_items is parsed string for each current row where each\n",
    "        # item in list is the date, open, high, low, close, and volume\n",
    "        row_items = individual_row.split('<span data-reactid=')[1].split('\"><!-- react-text: ')\n",
    "        \n",
    "        if len(row_items) > 1:\n",
    "            sum_item = row_items[0].split('>')[1].split('<')[0]\n",
    "            sum_value = row_items[1].split('-->')[1].split('<')[0]\n",
    "        elif 'YIELD' in row_items[0]:\n",
    "            try:\n",
    "                temp_val = row_items[0].split('-value\">')[1].split(\"</td>\")[0]\n",
    "                div_amount = float(temp_val.split(' ')[0])\n",
    "                div_yield = float(temp_val.split(' ')[1].replace('(','').replace(')','').replace('%',''))\n",
    "\n",
    "                sum_dict['Div'] = div_amount\n",
    "                sum_dict['Yield'] = div_yield\n",
    "            except:\n",
    "                sum_dict['Div'] = np.nan\n",
    "                sum_dict['Yield'] = np.nan\n",
    "        elif 'Market Cap' in row_items[0]:\n",
    "            sum_item = 'Market Cap (B)'\n",
    "            mkt_cap = row_items[0].split('data-reactid=\"')[-1].split('>')[1].split('<')[0]\n",
    "            mkt_cap_amount = float(mkt_cap[:-1])\n",
    "            if mkt_cap[-1] == 'M':\n",
    "                sum_value = mkt_cap_amount/1000\n",
    "            else:\n",
    "                sum_value = mkt_cap_amount\n",
    "        else:\n",
    "            sum_item = row_items[0].split('>')[1].split('<')[0]\n",
    "            sum_value = row_items[0].split('data-reactid=\"')[-1].split('>')[1].split('<')[0]\n",
    "        \n",
    "        sum_dict[sum_item] = sum_value\n",
    "    \n",
    "    sum_dict['Days Since Last Earnings'] = (dt.datetime.today().date() - \n",
    "                                            stock_earnings(ticker)['Earnings Dates'][1].date()).days\n",
    "\n",
    "    return pd.DataFrame(sum_dict, index = [ticker])\n",
    "\n",
    "# Function to return fundametal data of a ticker list\n",
    "def get_fundas(ticker_lst):\n",
    "    fund_lst = []\n",
    "    for tick in ticker_lst:\n",
    "        try:\n",
    "            fund_lst.append(fundamentals(tick))\n",
    "        except:\n",
    "            continue\n",
    "    return pd.concat(fund_lst,axis = 0)\n",
    "\n",
    "# Function historical data from alpha advantage\n",
    "def historical_data(ticker, day_number = 252, rolling_window = 20, outsize = 'full'):\n",
    "    alphavantage_link = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={0}&apikey=5HZEUI5AFJB06BUK&datatype=csv&outputsize={1}'.format(ticker, outsize)\n",
    "    stockframe = pd.read_csv(alphavantage_link, index_col = 0).sort_index()[['open', 'close']]\n",
    "    stockframe['daily_ret'] = np.log(stockframe['close']/stockframe['close'].shift(1))\n",
    "    stockframe['intra_ret'] = np.log(stockframe['close']/stockframe['open'])\n",
    "    stockframe['ovrnt_ret'] = np.log(stockframe['open']/stockframe['close'].shift(1))\n",
    "    stockframe['daily_vol'] = stockframe.daily_ret.rolling(window=rolling_window,center=False).std()\n",
    "    stockframe['intra_vol'] = stockframe.intra_ret.rolling(window=rolling_window,center=False).std()\n",
    "    stockframe['ovrnt_vol'] = stockframe.ovrnt_ret.rolling(window=rolling_window,center=False).std()\n",
    "    stockframe['daily_ann'] = stockframe.daily_vol*np.sqrt(252)\n",
    "    stockframe['intra_ann'] = stockframe.intra_vol*np.sqrt((24/6.5)*252)\n",
    "    stockframe['ovrnt_ann'] = stockframe.ovrnt_vol*np.sqrt((24/17.5)*252)\n",
    "    stockframe['oc_diff'] = stockframe.close - stockframe.open\n",
    "    stockframe['daily_dollar_vol'] = stockframe.daily_vol*stockframe.close.shift(1)\n",
    "    stockframe['daily_dollar_std'] = np.abs(stockframe.oc_diff/stockframe.daily_dollar_vol)\n",
    "\n",
    "    return stockframe.tail(day_number)\n",
    "\n",
    "# Function for building a dataframe of volatilities\n",
    "# Daily, Intraday, Overnight\n",
    "def current_volatility(ticker_list, roll = 20):\n",
    "    \n",
    "    rows = []\n",
    "    for tick in ticker_list:\n",
    "        curr_vol = historical_data(tick).tail(1)[['daily_ann','intra_ann','ovrnt_ann','close',\n",
    "                                                  'daily_dollar_vol']]\n",
    "        curr_vol.index.name = 'Tickers'\n",
    "        curr_vol.index = [tick]\n",
    "        rows.append(curr_vol)\n",
    "        \n",
    "    return pd.concat(rows, axis = 0)\n",
    "\n",
    "# Function for pulling S&P500 data and calculating volatilities\n",
    "def sp500_filter():\n",
    "    sp500 = pull_sp500_list()\n",
    "    sp500_vols = current_volatility(sp500.index.tolist())\n",
    "    df = pd.concat([sp500_vols, sp500], axis = 1).dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_raw_data(ticker):\n",
    "    tape = Options(ticker, 'yahoo')\n",
    "    data = tape.get_all_data()\n",
    "    return data\n",
    "\n",
    "# Function for pulling options for a given ticker\n",
    "def option_filter(ticker, moneyness_thresh, dte_thresh):\n",
    "    data = get_raw_data(ticker).reset_index()\n",
    "    data['Moneyness'] = np.abs(data['Strike'] - data['Underlying_Price'])/data['Underlying_Price']\n",
    "    data['DTE'] = (data['Expiry'] - dt.datetime.today()).dt.days\n",
    "    data = data[['Strike', 'DTE', 'Type', 'IV', 'Vol','Open_Int', 'Moneyness', 'Root', 'Underlying_Price',\n",
    "                 'Last','Bid','Ask']]\n",
    "    data['Mid'] = data['Ask'] - data['Bid']\n",
    "\n",
    "    filtered_data = data[(data['Moneyness'] <= moneyness_thresh) &\n",
    "                         (data['DTE'] <= dte_thresh)].reset_index()[data.columns]\n",
    "    put_ivs = filtered_data[filtered_data.Type == 'put'].pivot(index='Strike', columns='DTE', values='IV').dropna()\n",
    "    call_ivs = filtered_data[filtered_data.Type == 'put'].pivot(index='Strike', columns='DTE', values='IV').dropna()\n",
    "    hv_data = current_volatility([ticker])\n",
    "\n",
    "    put_ivs['Close'] = hv_data['close'][0]\n",
    "    call_ivs['Close'] = hv_data['close'][0]\n",
    "    put_ivs['Daily HV'] = hv_data['daily_ann'][0]\n",
    "    call_ivs['Daily HV'] = hv_data['daily_ann'][0]\n",
    "    put_ivs['Intra HV'] = hv_data['intra_ann'][0]\n",
    "    call_ivs['Intra HV'] = hv_data['intra_ann'][0]\n",
    "    put_ivs['Overnight HV'] = hv_data['ovrnt_ann'][0]\n",
    "    call_ivs['Overnight HV'] = hv_data['ovrnt_ann'][0]\n",
    "    put_ivs['Daily Dollar Vol'] = hv_data['daily_dollar_vol'][0]\n",
    "    call_ivs['Daily Dollar Vol'] = hv_data['daily_dollar_vol'][0]\n",
    "\n",
    "    call_ivs.index.name = ticker + ' Call Strike'\n",
    "    put_ivs.index.name = ticker + ' Put Strike'\n",
    "    return call_ivs, put_ivs\n",
    "\n",
    "\n",
    "def phase1_options_filter(ticker_lst, hv_day_roll, atm_distance, dte_thresh):\n",
    "    \n",
    "    pulled_options = []\n",
    "    calls = {}\n",
    "    puts = {}\n",
    "\n",
    "    moneyness_thresh = atm_distance\n",
    "\n",
    "    for tick in ticker_lst:\n",
    "        try:\n",
    "            call, put = option_filter(tick, moneyness_thresh, dte_thresh)\n",
    "\n",
    "            # Checking IV vs HV of the options chain out more than 2 weeks\n",
    "            iv_chain_to_check = list(filter(lambda x: x >= hv_day_roll, \n",
    "                                            list(filter(lambda x: type(x) == int,call.columns.tolist()))))[0]\n",
    "            average_difference_iv_hv = abs(call[iv_chain_to_check] - call['Intra HV']).mean()\n",
    "\n",
    "            if average_difference_iv_hv >= 0.15:\n",
    "                calls[tick] = call\n",
    "                puts[tick] = put\n",
    "                pulled_options.append(tick)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return pulled_options, calls, puts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tick_lst = active_stocks.head(100).index.tolist()\n",
    "days_since_earnings_threshold = 30\n",
    "mkt_cap_thresh = 5\n",
    "moneyness_thresh = 0.05\n",
    "hv_roll = 20\n",
    "dte_thresh = 30\n",
    "sort_on = 'ovrnt_ann'\n",
    "max_options_num = 10\n",
    "\n",
    "vols = current_volatility(tick_lst, roll = 20)\n",
    "vols = vols.sort_values(['intra_ann'], ascending = False).dropna()\n",
    "fundas = get_fundas(vols.index.tolist())\n",
    "tickers = pd.concat([vols,fundas],axis = 1)\n",
    "tickers = tickers[(tickers['Days Since Last Earnings'] >= days_since_earnings_threshold) &\n",
    "                  (tickers['Market Cap (B)'] >= mkt_cap_thresh) &\n",
    "                  (tickers['close'] >= 30)].sort_values([sort_on], ascending = False)\n",
    "\n",
    "pulled_options, pulled_calls, pulled_puts = phase1_options_filter(tickers.index.tolist()[:max_options_num], \n",
    "                                                                  hv_roll, moneyness_thresh, dte_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
