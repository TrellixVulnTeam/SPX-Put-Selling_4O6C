{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fang\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning:\n",
      "\n",
      "The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note to import from .py files, must follow structure\n",
    "# from <.py filename excluding '.py'> import <class name>\n",
    "\n",
    "# Importing necessary models\n",
    "import smtplib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas.stats.moments as st\n",
    "from pandas import ExcelWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.ticker as ticker\n",
    "from lxml import html\n",
    "import requests\n",
    "import webbrowser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "import csv\n",
    "import sched, time\n",
    "from pandas_datareader.data import Options\n",
    "from py_vollib.black_scholes_merton.implied_volatility import *\n",
    "import dash\n",
    "from dash.dependencies import Input, Output\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output, State\n",
    "import plotly.plotly as py\n",
    "import plotly\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import skewnorm as skn\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Using plotly api_key credentials\n",
    "# plotly.tools.set_credentials_file(username='aspiringfastlaner', api_key='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling all historical data and collapsing into raw dataframe for use\n",
    "The following cell has code to pull data from yahoo finance for GSPC, and from\n",
    "CBOE for VIX, VVIX, and SKEW.\n",
    "\n",
    "#### The final product of the cell is a dataframe named, df, that stores all raw historical yahoo finance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:95: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class datacollect:\n",
    "        \n",
    "    # Use six to import urllib so it is working for Python2/3\n",
    "    from six.moves import urllib\n",
    "    # If you don't want to use six, please comment out the line above\n",
    "    # and use the line below instead (for Python3 only).\n",
    "    #import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "    '''\n",
    "    Starting on May 2017, Yahoo financial has terminated its service on\n",
    "    the well used EOD data download without warning. This is confirmed\n",
    "    by Yahoo employee in forum posts.\n",
    "    Yahoo financial EOD data, however, still works on Yahoo financial pages.\n",
    "    These download links uses a \"crumb\" for authentication with a cookie \"B\".\n",
    "    This code is provided to obtain such matching cookie and crumb.\n",
    "    '''\n",
    "\n",
    "    # Build the cookie handler\n",
    "    cookier = urllib.request.HTTPCookieProcessor()\n",
    "    opener = urllib.request.build_opener(cookier)\n",
    "    urllib.request.install_opener(opener)\n",
    "\n",
    "    # Cookie and corresponding crumb\n",
    "    _cookie = None\n",
    "    _crumb = None\n",
    "\n",
    "    _headers={'User-Agent': 'Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11'}\n",
    "\n",
    "    def get_cookie_crumb():\n",
    "        '''\n",
    "        This function perform a query and extract the matching cookie and crumb.\n",
    "        '''\n",
    "\n",
    "        # Perform a Yahoo financial lookup on SP500\n",
    "        req = urllib.request.Request('https://finance.yahoo.com/quote/^GSPC', headers=_headers)\n",
    "        f = urllib.request.urlopen(req)\n",
    "        alines = f.read().decode('utf-8')\n",
    "\n",
    "        # Extract the crumb from the response\n",
    "        global _crumb\n",
    "        cs = alines.find('CrumbStore')\n",
    "        cr = alines.find('crumb', cs + 10)\n",
    "        cl = alines.find(':', cr + 5)\n",
    "        q1 = alines.find('\"', cl + 1)\n",
    "        q2 = alines.find('\"', q1 + 1)\n",
    "        crumb = alines[q1 + 1:q2]\n",
    "        _crumb = crumb\n",
    "\n",
    "        # Extract the cookie from cookiejar\n",
    "        global cookier, _cookie\n",
    "        for c in cookier.cookiejar:\n",
    "            if c.domain != '.yahoo.com':\n",
    "                continue\n",
    "            if c.name != 'B':\n",
    "                continue\n",
    "        _cookie = c.value\n",
    "\n",
    "        # Print the cookie and crumb\n",
    "        # print('Cookie:', _cookie)\n",
    "        # print('Crumb:', _crumb)\n",
    "        return _crumb\n",
    "\n",
    "    # Downloading directly from yahoo finance spx or vvix data\n",
    "    def yahoo_historical(ticker = 'SPX'):\n",
    "        # Using requests to ping yahoo finance to retrieve \n",
    "        # historical data table\n",
    "\n",
    "        # Getting cookie crumb for yahoo finance query\n",
    "        get_cookie_crumb()\n",
    "\n",
    "        if ticker == 'VVIX':\n",
    "            site = 'https://query1.finance.yahoo.com/v7/finance/download/%5EVVIX?period1=1167811200&period2=' + str(int(time.time())) + '&interval=1d&events=history&crumb=' + get_cookie_crumb().replace('\\\\','')\n",
    "        else:\n",
    "            site = 'https://query1.finance.yahoo.com/v7/finance/download/%5EGSPC?period1=-630950400&period2=' + str(int(time.time())) + '&interval=1d&events=history&crumb=' + get_cookie_crumb().replace('\\\\','')\n",
    "\n",
    "        df = pd.read_csv(site)\n",
    "        return df\n",
    "\n",
    "    # Reading in Data\n",
    "    # Reading VIX data from CBOE directly\n",
    "    # VIX is stored as 3 separate files on CBOE's website\n",
    "    #   2004 to present : http://www.cboe.com/publish/scheduledtask/mktdata/datahouse/vixcurrent.csv\n",
    "    #   1990 to 2003    : http://www.cboe.com/publish/scheduledtask/mktdata/datahouse/vixarchive.xls\n",
    "    #   1986 to 2003 VXO: http://www.cboe.com/publish/scheduledtask/mktdata/datahouse/vxoarchive.xls\n",
    "\n",
    "    # First read raw files directly \n",
    "    vix_present = pd.read_csv('http://www.cboe.com/publish/scheduledtask/mktdata/datahouse/vixcurrent.csv').dropna()\n",
    "    # vix_old = pd.read_excel('http://www.cboe.com/publish/scheduledtask/mktdata/datahouse/vixarchive.xls').dropna()\n",
    "    vxo_old = pd.read_excel('http://www.cboe.com/publish/scheduledtask/mktdata/datahouse/vxoarchive.xls').dropna()\n",
    "\n",
    "    # Function for cleaning CBOE VIX data\n",
    "    def clean_cboe(df):\n",
    "        df.columns = ['Date','Open','High','Low','Close']\n",
    "        df = df[1:]\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.set_index(pd.DatetimeIndex(df['Date']))\n",
    "        return df[['Open','High','Low','Close']]\n",
    "\n",
    "    # Applying clean_cboe to vix data\n",
    "    vix_present = clean_cboe(vix_present)\n",
    "    # vix_old = clean_cboe(vix_old)\n",
    "    vxo_old = clean_cboe(vxo_old)\n",
    "\n",
    "    # Currently the vix_old dataframe doesn't have the Open prices so VXO will be used to proxy VIX prior\n",
    "    # to 2003\n",
    "    vix = pd.concat([vxo_old,vix_present],axis = 0)\n",
    "\n",
    "    # Reading SKEW Index data directly from CBOE\n",
    "    skew = pd.read_csv('https://www.cboe.com/publish/scheduledtask/mktdata/datahouse/skewdailyprices.csv')\n",
    "    skew_raw = skew.copy()\n",
    "    skew.columns = ['Date','Skew','na1','na2']\n",
    "    skew = skew[1:]\n",
    "    skew['Date'] = pd.to_datetime(skew['Date'])\n",
    "    skew = skew.set_index(pd.DatetimeIndex(skew['Date']))[['Skew']]\n",
    "    skew['skew'] = -(pd.to_numeric(skew['Skew'], downcast='float') - 100)/10\n",
    "    del skew['Skew']\n",
    "\n",
    "    # Reading in SPX Data\n",
    "    spx = yahoo_historical()\n",
    "    spx = spx.set_index(pd.DatetimeIndex(spx['Date']))[['Open','High','Low','Close','Adj Close']]\n",
    "\n",
    "    # Reading in VVIX Data\n",
    "    # vvix = yahoo_historical('VVIX')\n",
    "    # vvix = vvix.set_index(pd.DatetimeIndex(vvix['Date']))[['Open','High','Low','Close','Adj Close']]\n",
    "\n",
    "    # Joining all index together to one dataframe\n",
    "    spx = spx[['Open','Close']]\n",
    "    spx.columns = ['SPX ' + s for s in spx.columns.tolist()]\n",
    "\n",
    "    vix = vix[['Open','Close']]\n",
    "    vix.columns = ['VIX ' + s for s in vix.columns.tolist()]\n",
    "\n",
    "    # vvix = vvix[['Open','Close']]\n",
    "    # vvix.columns = ['VVIX ' + s for s in vvix.columns.tolist()]\n",
    "\n",
    "    #\n",
    "    df = pd.concat([spx,vix,skew],axis = 1).dropna() # Currently excluding VVIX\n",
    "\n",
    "    # An error in data: 2000-10-18 VIX Close value is a string, converting to float\n",
    "    df['VIX Close'][2714] = 32.5\n",
    "\n",
    "    # Fixing VIX values so that they are floats\n",
    "    df['VIX Close'] = df['VIX Close'].astype('float')\n",
    "    df['VIX Open'] = df['VIX Open'].astype('float')\n",
    "\n",
    "    # Adjusting VIX so that it's on 252 trading days\n",
    "    df['Daily VIX Open'] = np.sqrt(((df['VIX Open']*df['VIX Open'])/365)*1.5)/100\n",
    "    df['Daily VIX Close'] = np.sqrt(((df['VIX Close']*df['VIX Close'])/365)*1.5)/100\n",
    "\n",
    "    # Cleaning up unused dataframes\n",
    "    del skew, spx, vix, vix_present, vxo_old\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Defined functions\n",
    "\n",
    "Below cell holds all necessary functions for VaR calculations\n",
    "- latest_yahoo: Pulls latest yahoo data for SPX, VIX, VVIX, and SKEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling Yahoo live data\n",
    "\n",
    "'''\n",
    "Function for pulling latest SPX, VIX, VVIX, or SKEW data. Input is a string, pulls \n",
    "the latest 2 lines of data from yahoo finance for given ticker and returns a \n",
    "dataframe of the open and close with the latest date as the first row.\n",
    "'''\n",
    "def latest_yahoo(ticker = 'SPX'):\n",
    "    # Using requests to ping yahoo finance to retrieve \n",
    "    # historical data table\n",
    "    if ticker == 'VIX':\n",
    "        site = 'https://finance.yahoo.com/quote/%5EVIX/history?p=^VIX'\n",
    "    elif ticker == 'VVIX':\n",
    "        site = 'https://finance.yahoo.com/quote/%5EVVIX/history?p=^VVIX'\n",
    "    elif ticker == 'SKEW':\n",
    "        site = 'https://finance.yahoo.com/quote/%5ESKEW/history?p=^SKEW'\n",
    "    else:\n",
    "        site = 'https://finance.yahoo.com/quote/%5EGSPC/history?p=^GSPC'\n",
    "        \n",
    "    res = requests.get(site)\n",
    "    soup = bs(res.text, 'lxml')\n",
    "    table = soup.find_all('table')[0]\n",
    "\n",
    "    # Initializing list to store date, open, and close values\n",
    "    # for GSPC\n",
    "    dates = []\n",
    "    opens = []\n",
    "    closes = []\n",
    "    \n",
    "    # Looping through the soup lxml text table format\n",
    "    # and splitting each row as a individual string\n",
    "    # and parsing string to retrieve the date,\n",
    "    # open, and close information.\n",
    "    i = 1\n",
    "    end_row = 3\n",
    "    for row in table.find_all('tr'):\n",
    "        # Individual row stores current row item and delimits on '\\n'\n",
    "        individual_row = str(row).split('\\n')\n",
    "        \n",
    "        # row_items is parsed string for each current row where each\n",
    "        # item in list is the date, open, high, low, close, and volume\n",
    "        row_items = [item.split('>')[1] for item in [string.split('</span>')[0] for string in individual_row[0].split('<span ')[1:]]]\n",
    "        \n",
    "        if i == 1:\n",
    "            # Skip first row because they are column headers\n",
    "            i += 1\n",
    "            continue\n",
    "        elif i == end_row:\n",
    "            break\n",
    "        else:\n",
    "            # Append necessary items to initialized lists for \n",
    "            # dataframe storage\n",
    "            dates.append(row_items[0])\n",
    "            opens.append(float(row_items[1].replace(',','')))\n",
    "            closes.append(float(row_items[5].replace(',','')))\n",
    "        i += 1\n",
    "    \n",
    "    # Return dataframe of necessary values\n",
    "    return pd.DataFrame({ticker + ' Open': opens,ticker + ' Close': closes}, index = dates)\n",
    "\n",
    "'''\n",
    "Helper function to pull all relevant current data from yahoo finance using\n",
    "latest_yahoo function call\n",
    "'''\n",
    "def yahoo_latest_data():\n",
    "    spx_current = latest_yahoo()['SPX Close'][0]\n",
    "    vix_current = latest_yahoo('VIX')['VIX Close'][0]\n",
    "    skew_current = latest_yahoo('SKEW')['SKEW Close'][0]\n",
    "    vvix_current = latest_yahoo('VVIX')['VVIX Close'][0]\n",
    "    return spx_current, vix_current, skew_current, vvix_current\n",
    "\n",
    "'''\n",
    "Function for calculating the single day implied VaR for the SP 500 index\n",
    "using VIX, SKEW, and the SPX spot.\n",
    "Inputs:\n",
    " - rolling_window [int] - for the number of days to expiry of put option\n",
    " - var_pct [float] - for the VaR level\n",
    " - option [string of length 1] - for put or call\n",
    "'''\n",
    "def implied_spx_var(rolling_window, var_pct, option = 'P'):\n",
    "    spx, vix, skew, vvix = yahoo_latest_data()\n",
    "    \n",
    "    alpha = -(skew - 100)/10\n",
    "    period_vix = (np.sqrt(((vix*vix)/365)*1.5)/100)*np.sqrt(rolling_window)\n",
    "    if option == 'C':\n",
    "        var_pct = 1 - var_pct\n",
    "        pct_var = norm.ppf(var_pct, 0, period_vix)\n",
    "    else:\n",
    "        pct_var = skn.ppf(var_pct, alpha, 0, period_vix)\n",
    "    strike_suggestion = spx*np.exp(pct_var)#(1 + pct_var)\n",
    "    # print('VaR return percent for SPX is: ' + str(round(pct_var*100,2)))\n",
    "    # print('Suggested SPX strike: ' + str(np.floor(spx_k_suggestion)))\n",
    "    var_spx_return = str(round(pct_var*100,2))\n",
    "    \n",
    "    return strike_suggestion, var_spx_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [15/Feb/2018 12:03:13] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Feb/2018 12:03:14] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Feb/2018 12:03:14] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Feb/2018 12:03:14] \"GET /favicon.ico HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Feb/2018 12:03:19] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Feb/2018 12:03:23] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Dash browser application\n",
    "\n",
    "app = dash.Dash()\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "                html.Label('Days to Expiry:'),\n",
    "                dcc.Input(id='dte-input-state', type='text', value='1'),\n",
    "            ],\n",
    "                className='six columns',\n",
    "            ),\n",
    "    html.Div([\n",
    "                html.Label('VaR Threshold:'),\n",
    "                dcc.Input(id='var-input-state', type='text', value='0.0005'),\n",
    "            ],\n",
    "                className='six columns',\n",
    "            ),\n",
    "    html.Button(id='submit-button', n_clicks=0, children='Submit'),\n",
    "    html.Div(id='output-state')\n",
    "])\n",
    "\n",
    "\n",
    "@app.callback(Output('output-state', 'children'),\n",
    "              [Input('submit-button', 'n_clicks')],\n",
    "              [State('dte-input-state', 'value'),\n",
    "               State('var-input-state', 'value')])\n",
    "def update_output(n_clicks, input1, input2):\n",
    "    strike_suggestion, var_spx_return = implied_spx_var(float(input1), float(input2), option = 'P')\n",
    "    \n",
    "    return u'''\n",
    "        Submitted {} times \\n\n",
    "        Suggested SPX strike: {} \\n\n",
    "        VaR return percent for SPX is: {}%\n",
    "    '''.format(n_clicks, strike_suggestion, var_spx_return)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
